{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3bbff7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import cv2\n",
    "import random\n",
    "import h5py\n",
    "\n",
    "import sys\n",
    "# sys.path.append('/kaggle/working/Depth-Anything-V2/metric_depth')\n",
    "\n",
    "from accelerate import Accelerator\n",
    "from accelerate.utils import set_seed\n",
    "from accelerate import notebook_launcher\n",
    "from accelerate import DistributedDataParallelKwargs\n",
    "\n",
    "import transformers\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision.transforms import v2\n",
    "from torchvision.transforms import Compose\n",
    "import torch.nn.functional as F\n",
    "import albumentations as A\n",
    "\n",
    "# from depth_anything_v2.dpt import DepthAnythingV2\n",
    "from depth_model.fdepth_resnet_v2 import FastDepthV2\n",
    "\n",
    "\n",
    "from metric_depth.util.loss import SiLogLoss\n",
    "from metric_depth.dataset.transform import Resize, NormalizeImage, PrepareForNet, Crop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0ace6fd",
   "metadata": {},
   "source": [
    "### List of paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0134ab2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_files(directory):\n",
    "    all_files = []\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            all_files.append(os.path.join(root, file))\n",
    "    return all_files\n",
    "\n",
    "\n",
    "train_paths = get_all_files('/kaggle/input/nyu-depth-dataset-v2/nyudepthv2/train')\n",
    "val_paths = get_all_files('/kaggle/input/nyu-depth-dataset-v2/nyudepthv2/val')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fecb0df5",
   "metadata": {},
   "source": [
    "### Dataset preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faee8be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#NYU Depth V2 40k. Original NYU is 400k\n",
    "class NYU(torch.utils.data.Dataset):\n",
    "    def __init__(self, paths, mode, size=(518, 518)):\n",
    "        \n",
    "        self.mode = mode #train or val\n",
    "        self.size = size\n",
    "        self.paths = paths\n",
    "        \n",
    "        net_w, net_h = size\n",
    "        #author's transforms\n",
    "        self.transform = Compose([\n",
    "            Resize(\n",
    "                width=net_w,\n",
    "                height=net_h,\n",
    "                resize_target=True if mode == 'train' else False,\n",
    "                keep_aspect_ratio=True,\n",
    "                ensure_multiple_of=14,\n",
    "                resize_method='lower_bound',\n",
    "                image_interpolation_method=cv2.INTER_CUBIC,\n",
    "            ),\n",
    "            NormalizeImage(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "            PrepareForNet(),\n",
    "        ] + ([Crop(size[0])] if self.mode == 'train' else []))\n",
    "        \n",
    "        # only horizontal flip in the paper\n",
    "        self.augs = A.Compose([\n",
    "            A.HorizontalFlip(),\n",
    "            A.ColorJitter(hue = 0.1, contrast=0.1, brightness=0.1, saturation=0.1),\n",
    "            A.GaussNoise(var_limit=25),\n",
    "#             A.ShiftScaleRotate(shift_limit=0.04, scale_limit=0.1, rotate_limit=7, interpolation=cv2.INTER_CUBIC, border_mode=0)\n",
    "        ])\n",
    "    \n",
    "    def __getitem__(self, item):\n",
    "        path = self.paths[item]\n",
    "        image, depth = self.h5_loader(path)\n",
    "        \n",
    "        if self.mode == 'train':\n",
    "            augmented = self.augs(image=image, mask = depth)\n",
    "            image = augmented[\"image\"] / 255.0\n",
    "            depth = augmented['mask']\n",
    "        else:\n",
    "            image = image / 255.0\n",
    "\n",
    "          \n",
    "        sample = self.transform({'image': image, 'depth': depth})\n",
    "\n",
    "        sample['image'] = torch.from_numpy(sample['image'])\n",
    "        sample['depth'] = torch.from_numpy(sample['depth'])\n",
    "        \n",
    "        # sometimes there are masks for valid depths in datasets because of noise e.t.c\n",
    "#         sample['valid_mask'] = ...\n",
    "\n",
    "#         sample['image_path'] = path\n",
    "        \n",
    "        return sample\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.paths)\n",
    "    \n",
    "    def h5_loader(self, path):\n",
    "        h5f = h5py.File(path, \"r\")\n",
    "        rgb = np.array(h5f['rgb'])\n",
    "        rgb = np.transpose(rgb, (1, 2, 0))\n",
    "        depth = np.array(h5f['depth'])\n",
    "        return rgb, depth"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b474addd",
   "metadata": {},
   "source": [
    "### Dataset demonstration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1daa90a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_images = 5\n",
    "\n",
    "fig, axes = plt.subplots(num_images, 2, figsize=(10, 5 * num_images))\n",
    "\n",
    "train_set = NYU(train_paths, mode='train') \n",
    "\n",
    "for i in range(num_images):\n",
    "    sample = train_set[i*1000]\n",
    "    img, depth = sample['image'].numpy(), sample['depth'].numpy()\n",
    "\n",
    "    mean = np.array([0.485, 0.456, 0.406]).reshape((3, 1, 1))\n",
    "    std = np.array([0.229, 0.224, 0.225]).reshape((3, 1, 1))\n",
    "    img = img*std+mean\n",
    "\n",
    "    axes[i, 0].imshow(np.transpose(img, (1,2,0)))\n",
    "    axes[i, 0].set_title('Image')\n",
    "    axes[i, 0].axis('off')\n",
    "\n",
    "\n",
    "    im1 = axes[i, 1].imshow(depth, cmap='viridis', vmin=0)\n",
    "    axes[i, 1].set_title('True Depth')\n",
    "    axes[i, 1].axis('off')\n",
    "    fig.colorbar(im1, ax=axes[i, 1])\n",
    "    \n",
    "\n",
    "\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e1fafa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataloaders(batch_size):\n",
    "    \n",
    "    train_dataset = NYU(train_paths, mode='train')\n",
    "    val_dataset = NYU(val_paths, mode='val')\n",
    "    \n",
    "    \n",
    "    train_dataloader = torch.utils.data.DataLoader(train_dataset, \n",
    "                                                  batch_size = batch_size,\n",
    "                                                  shuffle=True,\n",
    "                                                  num_workers=4,\n",
    "                                                  drop_last=True\n",
    "#                                                   pin_memory=True\n",
    "                                                  )\n",
    "\n",
    "    val_dataloader = torch.utils.data.DataLoader(val_dataset, \n",
    "                                               batch_size = 1, #for dynamic resolution evaluations without padding\n",
    "                                               shuffle=False,\n",
    "                                               num_workers=4,\n",
    "                                               drop_last=True\n",
    "#                                                 pin_memory=True\n",
    "                                                )\n",
    "    \n",
    "    return train_dataloader, val_dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd59b675",
   "metadata": {},
   "source": [
    "### Metrics function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6caab21f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_depth(pred, target):\n",
    "    assert pred.shape == target.shape\n",
    "\n",
    "    thresh = torch.max((target / pred), (pred / target))\n",
    "\n",
    "    d1 = torch.sum(thresh < 1.25).float() / len(thresh)\n",
    "\n",
    "    diff = pred - target\n",
    "    diff_log = torch.log(pred) - torch.log(target)\n",
    "\n",
    "    abs_rel = torch.mean(torch.abs(diff) / target)\n",
    "\n",
    "    rmse = torch.sqrt(torch.mean(torch.pow(diff, 2)))\n",
    "    mae = torch.mean(torch.abs(diff))\n",
    "\n",
    "    silog = torch.sqrt(torch.pow(diff_log, 2).mean() - 0.5 * torch.pow(diff_log.mean(), 2))\n",
    "\n",
    "    return {'d1': d1.detach(), 'abs_rel': abs_rel.detach(),'rmse': rmse.detach(), 'mae': mae.detach(), 'silog':silog.detach()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c62411b",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a6a80cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fn():\n",
    "    \n",
    "    device = \"cuda:0\"\n",
    "    load_state = False\n",
    "    state_path = './'\n",
    "\n",
    "    #config nyu\n",
    "    batch_size = 128\n",
    "    max_depth = 10\n",
    "\n",
    "\n",
    "    \n",
    "    # params\n",
    "    num_epochs = 500\n",
    "    warmup_epochs = 8\n",
    "    num_cycles = 2\n",
    "\n",
    "    print(\"CUDA available:\", torch.cuda.is_available())\n",
    "    print(\"CUDA device:\", torch.cuda.current_device())\n",
    "    print(\"Device name:\", torch.cuda.get_device_name(torch.cuda.current_device()))\n",
    "\n",
    "\n",
    "\n",
    "    model = FastDepthV2(max_depth=max_depth).to(device)\n",
    "\n",
    "    # optim = torch.optim.Adam(\n",
    "    #       model.parameters(),  # lấy toàn bộ parameter của model\n",
    "    #       lr=3e-4,\n",
    "    #       weight_decay=0.01\n",
    "    #   )\n",
    "    \n",
    "    backbone_params = model.encoder.parameters()\n",
    "    decoder_params = model.decoder.parameters()\n",
    "\n",
    "    optim = torch.optim.Adam([\n",
    "        {\"params\": backbone_params, \"lr\": 3e-4},  # backbone LR nhỏ\n",
    "        {\"params\": decoder_params, \"lr\": 3e-3}    # decoder LR lớn\n",
    "    ], weight_decay=1e-5)\n",
    "\n",
    "\n",
    "    print('Model created')\n",
    "\n",
    "    criterion = SiLogLoss() # author's loss\n",
    "    # criterion = SiLogL1Loss()\n",
    "    # criterion = DepthLoss()\n",
    "    # scheduler = transformers.get_cosine_schedule_with_warmup(optim, len(train_dataloader)*warmup_epochs, num_epochs*scheduler_rate*len(train_dataloader))\n",
    "\n",
    "    # train_loader, val_loader = dataloader_v6.create_data_loaders(\"/home/gremsy_guest/hyp_workspace/depth_dataset/datasets/hyp_dataset_v1\", batch_size=512, size=(160, 128))\n",
    "    train_dataloader, val_dataloader = get_dataloaders(batch_size)\n",
    "\n",
    "    print(f\"size of train loader: {len(train_dataloader)}; val loader: {len(val_dataloader)}\")\n",
    " \n",
    "    # best val monitor: loss silog\n",
    "    best_val = 1e9\n",
    "    # best_loss = 1e9\n",
    "    history = {\"train_loss\": [], \"val_loss\": [], \"val_metrics\": []}\n",
    "\n",
    "    if load_state:\n",
    "        checkpoint = torch.load(\"/home/gremsy_guest/hyp_workspace/depth_v2/ours_checkpoints/11/last_checkpoint_16.pth\", map_location=device)\n",
    "        model.load_state_dict(checkpoint[\"model\"])\n",
    "        optim.load_state_dict(checkpoint[\"optim\"])\n",
    "\n",
    "        # model.load_state_dict(checkpoint)\n",
    "        model = model.to(device)\n",
    "\n",
    "\n",
    "    # model = torch.compile(model)  \n",
    "\n",
    "    \n",
    "\n",
    "    # Chọn device\n",
    "    # device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    # print(f\"Using device: {device}\")\n",
    "    # for name, param in model.named_parameters():\n",
    "    #     print(name, param.device)\n",
    "\n",
    "    # print(\"------------------------------------------------------------------\")\n",
    "\n",
    "\n",
    "    for epoch in range(0, num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "\n",
    "        for i , sample in enumerate(tqdm(train_dataloader, total=len(train_dataloader))):\n",
    "            # img, depth = input.to(device), target.to(device)\n",
    "            img, depth = sample['image'], sample['depth']\n",
    "            img, depth = img.to(device), depth.to(device)\n",
    "\n",
    "            optim.zero_grad()\n",
    "            pred = model(img)\n",
    "\n",
    "            # loss = criterion('l1',pred,depth,epoch)\n",
    "\n",
    "            # mask = (depth > 1e-3)\n",
    "            mask = (depth > 1e-3) & (depth <= max_depth) & torch.isfinite(depth)\n",
    "\n",
    "            # print(\"pred shape:\", pred.shape)\n",
    "            # print(\"target shape:\", target.shape)\n",
    "            # print(\"valid_mask shape:\", mask.shape)\n",
    "            loss = criterion(pred, depth, mask)\n",
    "\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "            # scheduler.step()\n",
    "\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        avg_loss = total_loss / len(train_dataloader)\n",
    "\n",
    "        # ===== Validation =====\n",
    "        model.eval()\n",
    "        # results = {'d1': 0, 'rmse': 0}\n",
    "        results = {'d1': 0, 'abs_rel': 0, 'rmse': 0, 'mae': 0, 'silog': 0}\n",
    "        # test_loss = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for sample , (input,target) in tqdm(enumerate(val_dataloader)):\n",
    "                # img, depth = input.to(device), target.to(device)\n",
    "                img, depth = sample['image'], sample['depth']\n",
    "                img, depth = img.to(device), depth.to(device)\n",
    "\n",
    "                pred = model(img)\n",
    "\n",
    "                pred = F.interpolate(pred[:, None], depth.shape[-2:], mode='bilinear', align_corners=True)[0, 0]\n",
    "\n",
    "                # test_loss += criterion('l1',pred, depth).item()\n",
    "                # pred = pred.squeeze(1).squeeze(0)\n",
    "\n",
    "                # mask = (depth >= 0.001)\n",
    "                # cur_results = eval_depth(pred, depth)\n",
    "\n",
    "                # print(depth)\n",
    "\n",
    "\n",
    "                mask = (depth > 1e-3) & (depth <= max_depth) & torch.isfinite(depth)\n",
    "\n",
    "                # print(mask)\n",
    "\n",
    "                # valid_pixels = mask.sum().item()\n",
    "                # print(f\"mask: {valid_pixels}\")\n",
    "\n",
    "                # print(\"pred shape:\", pred.shape)\n",
    "                # print(\"target shape:\", target.shape)\n",
    "                # print(\"valid_mask shape:\", mask.shape)\n",
    "                cur_results = eval_depth(pred[mask], depth[mask])\n",
    "\n",
    "\n",
    "                for k in results:\n",
    "                    results[k] += cur_results[k]\n",
    "\n",
    "        \n",
    "        # val_loss = test_loss/len(val_loader)\n",
    "\n",
    "        # for k in results:\n",
    "        #    results[k] = round(results[k] / len(val_loader), 3)\n",
    "        for k in results:\n",
    "            results[k] = round((results[k] / len(val_dataloader)).item(), 3)\n",
    "\n",
    "        # # ===== Save Checkpoint =====\n",
    "        # torch.save({\n",
    "        #     \"model\": model.state_dict(),\n",
    "        #     \"optim\": optim.state_dict(),\n",
    "        #     # \"scheduler\": scheduler.state_dict()\n",
    "        # }, f\"{state_path}/last_checkpoint_{epoch}.pth\")\n",
    "\n",
    "        # if results['abs_rel'] < best_val_absrel:\n",
    "        if results['abs_rel'] < best_val:\n",
    "\n",
    "            best_val = results['abs_rel']\n",
    "            new_ckpt = f\"{state_path}/checkpoint_best_{epoch}.pth\"\n",
    "\n",
    "            # 1. Lưu checkpoint mới\n",
    "            # torch.save(model.state_dict(), new_ckpt)\n",
    "            torch.save({\n",
    "                \"model\": model.state_dict(),\n",
    "                \"optim\": optim.state_dict(),\n",
    "                # \"scheduler\": scheduler.state_dict()\n",
    "            }, new_ckpt)\n",
    "\n",
    "\n",
    "        # Cập nhật history\n",
    "        history[\"train_loss\"].append(avg_loss)\n",
    "        # history[\"val_loss\"].append(val_loss)\n",
    "        history[\"val_metrics\"].append(results)\n",
    "\n",
    "        print(f\"epoch_{epoch}, train_loss={avg_loss:.5f}, val_metrics={results}\")\n",
    "\n",
    "        # ==== Vẽ biểu đồ ====\n",
    "        # epochs = range(1, num_epochs+1)\n",
    "        epochs = range(1, len(history[\"train_loss\"]) + 1)\n",
    "        loss_val = [m[\"silog\"] for m in history[\"val_metrics\"]]  # lấy metric silog từ val_metrics\n",
    "\n",
    "        plt.figure(figsize=(8, 5))\n",
    "\n",
    "        # Train loss\n",
    "        plt.plot(epochs, history[\"train_loss\"], label=\"Train Loss\", marker='o')\n",
    "\n",
    "        # Validation loss\n",
    "        plt.plot(epochs, loss_val, label=\"Val Loss\", marker='s')\n",
    "\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        plt.title(\"Training vs Validation Loss\")\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "\n",
    "        # Lưu biểu đồ\n",
    "        plt.savefig(f\"{state_path}/train_val_loss_curve.png\", dpi=150)\n",
    "        plt.close()\n",
    "\n",
    "        absrel = [m[\"abs_rel\"] for m in history[\"val_metrics\"]]\n",
    "        plt.figure(figsize=(8,5))\n",
    "        plt.plot(epochs, absrel, label=\"AbsRel (val)\")\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(\"AbsRel\")\n",
    "        plt.legend()\n",
    "        plt.savefig(f\"{state_path}/val_absrel_curve.png\")\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45e5a399",
   "metadata": {},
   "outputs": [],
   "source": [
    "#You can run this code with 1 gpu. Just set num_processes=1\n",
    "notebook_launcher(train_fn, num_processes=1)\n",
    "# ignore the error. it's harmless"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ee0aa02",
   "metadata": {},
   "source": [
    "### Inference\n",
    "\n",
    "Reminder: This is ABSOLUTE (metric) depth, not a relative depth!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc39a4a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FastDepthV2(max_depth=10).to('cuda:0')\n",
    "checkpoint = torch.load(\"\", map_location='cuda:0')\n",
    "model.load_state_dict(checkpoint[\"model\"])\n",
    "model = model.to('cuda:0')\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97f88074",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_images = 10\n",
    "\n",
    "fig, axes = plt.subplots(num_images, 3, figsize=(15, 5 * num_images))\n",
    "\n",
    "val_dataset = NYU(val_paths, mode='val') \n",
    "model.eval()\n",
    "for i in range(num_images):\n",
    "    sample = val_dataset[i]\n",
    "    img, depth = sample['image'], sample['depth']\n",
    "    \n",
    "    mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)\n",
    "    std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)\n",
    "   \n",
    "    with torch.inference_mode():\n",
    "        pred = model(img.unsqueeze(0).to('cuda'))\n",
    "        pred = F.interpolate(pred[:, None], depth.shape[-2:], mode='bilinear', align_corners=True)[0, 0]\n",
    "            \n",
    "    img = img*std + mean\n",
    "     \n",
    "    axes[i, 0].imshow(img.permute(1,2,0))\n",
    "    axes[i, 0].set_title('Image')\n",
    "    axes[i, 0].axis('off')\n",
    "\n",
    "    max_depth = max(depth.max(), pred.cpu().max())\n",
    "    \n",
    "    im1 = axes[i, 1].imshow(depth, cmap='viridis', vmin=0, vmax=max_depth)\n",
    "    axes[i, 1].set_title('True Depth')\n",
    "    axes[i, 1].axis('off')\n",
    "    fig.colorbar(im1, ax=axes[i, 1])\n",
    "    \n",
    "    im2 = axes[i, 2].imshow(pred.cpu(), cmap='viridis', vmin=0, vmax=max_depth)\n",
    "    axes[i, 2].set_title('Predicted Depth')\n",
    "    axes[i, 2].axis('off')\n",
    "    fig.colorbar(im2, ax=axes[i, 2])\n",
    "\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f4d0cc3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff018c3b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b53e2c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a91450e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce2c368a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64f367f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc4d8de0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
